{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc8URf-s_zy8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaX0BBj2BD94",
        "outputId": "80211b01-08e5-4c15-cfab-f497def84609"
      },
      "outputs": [],
      "source": [
        "url = 'https://drive.google.com/uc?id=1yIDmIeifB2_rwl6U45F23AAgnpSmnkbc'\n",
        "\n",
        "# unzip the file and store dataset in recipe_box/\n",
        "output = 'recipe_box.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "extract_to = 'recipe_box'\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-zs_fJUhBWX"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Remove the string \"ADVERTISEMENT\"\n",
        "    cleaned_text = re.sub(r'ADVERTISEMENT|\\s+', ' ', text).strip()\n",
        "    return cleaned_text\n",
        "\n",
        "def clean_dataset(dataset):\n",
        "    for recipe in dataset:\n",
        "        if 'ingredients' in recipe:\n",
        "            recipe['ingredients'] = [clean_text(ingredient) for ingredient in recipe['ingredients'] if ingredient]\n",
        "        if 'instructions' in recipe and recipe['instructions']:\n",
        "            recipe['instructions'] = clean_text(recipe['instructions'])\n",
        "    return dataset\n",
        "\n",
        "def recipe_validate_required_fields(recipe):\n",
        "    required_keys = ['title', 'ingredients', 'instructions']\n",
        "    if not recipe:\n",
        "        return False\n",
        "    for required_key in required_keys:\n",
        "        if not recipe.get(required_key):\n",
        "            return False\n",
        "        if isinstance(recipe[required_key], list) and not recipe[required_key]:\n",
        "            return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plKDXbUqDcA_",
        "outputId": "5c2bc44e-7ac7-44f4-fa10-f162a9d82b44"
      },
      "outputs": [],
      "source": [
        "def load_dataset():\n",
        "  dataset_file_names = [\n",
        "    'recipes_raw_nosource_ar.json',\n",
        "    # 'recipes_raw_nosource_epi.json',\n",
        "    # 'recipes_raw_nosource_fn.json',\n",
        "  ]\n",
        "\n",
        "  dataset = []\n",
        "  for dataset_file_name in dataset_file_names:\n",
        "    dataset_file_path = os.path.join('recipe_box', dataset_file_name)\n",
        "    with open(dataset_file_path, 'r') as dataset_file:\n",
        "      json_data_dict = json.load(dataset_file)\n",
        "      json_data_list = list(json_data_dict.values())\n",
        "      json_data_list = [recipe for recipe in json_data_list if recipe_validate_required_fields(recipe)]\n",
        "      cleaned_data = clean_dataset(json_data_list)\n",
        "      dataset.extend(cleaned_data)\n",
        "      print_dataset_info(dataset_file_path, cleaned_data)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def print_dataset_info(file_path, json_data):\n",
        "  print(file_path)\n",
        "  print('samples size:', len(json_data))\n",
        "  if json_data:\n",
        "    print('title:', json_data[0]['title'])\n",
        "    print('ingredients:', json_data[0]['ingredients'])\n",
        "    print('instructions:', json_data[0]['instructions'])\n",
        "    print()\n",
        "\n",
        "dataset_raw = load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0I-rvytLHGL",
        "outputId": "950ab0fc-9bd4-4f09-e85d-713531462b78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (31617, 500)\n",
            "y_train shape: (31617, 500)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 5000\n",
        "max_sequence_length = 500  # Maximum length of sequences\n",
        "\n",
        "# Tokenizer configuration\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "all_texts = [recipe['ingredients'] + [recipe['instructions']] for recipe in dataset_raw]\n",
        "all_texts = [' '.join(texts) for texts in all_texts]  # Combine ingredients and instructions for tokenization\n",
        "tokenizer.fit_on_texts(all_texts)\n",
        "\n",
        "# Function to prepare sequences\n",
        "def prepare_sequences(texts, tokenizer, max_sequence_length):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "    return padded_sequences\n",
        "\n",
        "# Preparing data\n",
        "ingredients_seqs = [' '.join(recipe['ingredients']) for recipe in dataset_raw]\n",
        "instructions_seqs = [recipe['instructions'] for recipe in dataset_raw]\n",
        "\n",
        "X = prepare_sequences(ingredients_seqs, tokenizer, max_sequence_length)\n",
        "y = prepare_sequences(instructions_seqs, tokenizer, max_sequence_length)\n",
        "\n",
        "# Splitting data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"X_train shape:\", X_train.shape)  # Should be (num_samples, sequence_length)\n",
        "print(\"y_train shape:\", y_train.shape)  # Should also be (num_samples, sequence_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gk3EMNJk3SO",
        "outputId": "f0c4f812-fcc9-4856-aaac-5713f1824605"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Model configuration\n",
        "\n",
        "embedding_dim = 256\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_sequence_length),\n",
        "    LSTM(256, return_sequences=True),\n",
        "    # LSTM(32, return_sequences=True),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYmYM9jpk6KW",
        "outputId": "bdb45b37-2204-4678-ed9a-d60003c48475"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
        "                    validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZDnmB7LrZg8",
        "outputId": "18c9ba60-54ed-4aa9-9e45-be104b147013"
      },
      "outputs": [],
      "source": [
        "sample_ingredients = \"chicken egg butter\"\n",
        "\n",
        "sample_seq = tokenizer.texts_to_sequences([sample_ingredients])\n",
        "sample_padded = pad_sequences(sample_seq, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "prediction = model.predict(sample_padded)\n",
        "\n",
        "predicted_instruction_idx = prediction.argmax(axis=-1)[0]\n",
        "predicted_instruction = tokenizer.sequences_to_texts([predicted_instruction_idx])\n",
        "\n",
        "print(\"Generated Cooking Instructions:\")\n",
        "print(predicted_instruction)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
