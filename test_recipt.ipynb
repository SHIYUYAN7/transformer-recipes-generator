{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinghe/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from MyLSTM import MyLSTM\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(model, tokenizer, ingredients, max_length=128):\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "\n",
    "    # Process ingredients into a single string if list, and tokenize\n",
    "    ingredients = ' '.join(ingredients) if isinstance(ingredients, list) else ingredients\n",
    "    input_ids = tokenizer.encode(ingredients, return_tensors='pt')\n",
    "\n",
    "    # Prepare initial hidden state and cell state for LSTM\n",
    "    # Initialize for all layers: num_layers, batch_size, hidden_dim\n",
    "    h, c = [torch.zeros(model.lstm.num_layers, 1, model.lstm.hidden_size) for _ in range(2)]  # Adjusted for all layers\n",
    "\n",
    "    # Prepare the sequence for generated ids, starting with the initial input\n",
    "    generated_ids = input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output, (h, c) = model.lstm(model.embedding(generated_ids[:, -1:]), (h, c))\n",
    "            output = model.fc(output.squeeze(1))  # Assuming model.fc is the final fully connected layer\n",
    "            next_token_id = torch.argmax(output, dim=1).unsqueeze(1)\n",
    "\n",
    "            generated_ids = torch.cat((generated_ids, next_token_id), dim=1)\n",
    "\n",
    "            # Check if the next token is the separator token, indicating the end\n",
    "            if next_token_id.item() == tokenizer.sep_token_id:\n",
    "                break\n",
    "\n",
    "    # Decode the sequence of token ids to a string\n",
    "    generated_text = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
    "    print('Generated IDs:', generated_ids.squeeze())\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"recipes_20000.xlsx\")\n",
    "training_data = data.head(1000)\n",
    "max_length = 128\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "input_ids = []\n",
    "output_ids = []\n",
    "\n",
    "for index, row in training_data.iterrows():\n",
    "    # string (list shape) to string(space split)\n",
    "    ingredients = (' ').join(eval(row['ingredients']))\n",
    "    steps = (' ').join(eval(row['steps']))\n",
    "\n",
    "    # tokenized input and output\n",
    "    input_tokens = tokenizer(ingredients, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length)\n",
    "    output_tokens = tokenizer(steps, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "    # add label\n",
    "    input_ids.append(input_tokens['input_ids'])\n",
    "    output_ids.append(output_tokens['input_ids'])\n",
    "\n",
    "    # convert to torch\n",
    "    input_ids_tensor = torch.stack(input_ids)\n",
    "    output_ids_tensor = torch.stack(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim = 128\n",
    "# hidden_dim = 256\n",
    "# num_layers = 5\n",
    "# dropout_rate = 0.1\n",
    "# batch_size = 64\n",
    "# epochs = 5\n",
    "\n",
    "# # Model\n",
    "# model_lstm = EnhancedLSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate)\n",
    "\n",
    "# dataset = TensorDataset(input_ids_tensor, output_ids_tensor)\n",
    "# train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "\n",
    "# # Training\n",
    "# model_lstm.train()\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = 0\n",
    "#     for inputs, targets in train_loader:\n",
    "#         inputs = inputs.squeeze(1)  # Remove the singleton dimension if present\n",
    "#         targets = targets.squeeze(1)  # Similarly, ensure targets are correctly shaped\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model_lstm(inputs) \n",
    "#         loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     average_loss = total_loss / len(train_loader)\n",
    "#     print(f'Epoch {epoch+1}, Average Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for MyLSTM\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 5\n",
    "dropout_rate = 0.1\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "# Model\n",
    "model_mylstm = MyLSTM(units=vocab_size, input_size=embedding_dim)\n",
    "\n",
    "dataset = TensorDataset(input_ids_tensor, output_ids_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_mylstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mylstm.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.float()  \n",
    "        targets = targets.squeeze(1) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model_mylstm(inputs) \n",
    "        outputs = outputs[-1]  \n",
    "\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Average Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch.nn.LSTM\n",
    "class RecipeLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate=0.1):\n",
    "        super(RecipeLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        final_output = self.fc(lstm_out)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for RecipeLSTM\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 5\n",
    "dropout_rate = 0.1\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "# Model\n",
    "model = RecipeLSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate)\n",
    "\n",
    "# Data\n",
    "dataset = TensorDataset(input_ids_tensor, output_ids_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 7.9862\n",
      "Epoch 2, Average Loss: 5.4272\n",
      "Epoch 3, Average Loss: 5.2742\n",
      "Epoch 4, Average Loss: 5.2556\n",
      "Epoch 5, Average Loss: 5.2321\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.squeeze(1)  # Remove the singleton dimension if present\n",
    "        targets = targets.squeeze(1)  # Similarly, ensure targets are correctly shaped\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Average Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated IDs: tensor([  101, 20949,  2417,  4330, 11565, 20548, 18856, 21818,  2015,   102,\n",
      "          101,   101,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "ingredients_list = ['onion', 'red bell pepper', 'garlic cloves']\n",
    "\n",
    "# generate\n",
    "recipe_text = generate_recipe(model, tokenizer, ingredients_list)\n",
    "# print(recipe_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'onion red bell pepper garlic cloves'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
