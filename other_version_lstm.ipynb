{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc8URf-s_zy8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=1yIDmIeifB2_rwl6U45F23AAgnpSmnkbc'\n",
        "\n",
        "# unzip the file and store dataset in recipe_box/\n",
        "output = 'recipe_box.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "extract_to = 'recipe_box'\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaX0BBj2BD94",
        "outputId": "80211b01-08e5-4c15-cfab-f497def84609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1yIDmIeifB2_rwl6U45F23AAgnpSmnkbc\n",
            "From (redirected): https://drive.google.com/uc?id=1yIDmIeifB2_rwl6U45F23AAgnpSmnkbc&confirm=t&uuid=97e0a1f3-40ef-4375-993a-edd306ffe5de\n",
            "To: /content/recipe_box.zip\n",
            "100%|██████████| 53.4M/53.4M [00:01<00:00, 27.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove the string \"ADVERTISEMENT\"\n",
        "    cleaned_text = re.sub(r'ADVERTISEMENT|\\s+', ' ', text).strip()\n",
        "    return cleaned_text\n",
        "\n",
        "def clean_dataset(dataset):\n",
        "    for recipe in dataset:\n",
        "        if 'ingredients' in recipe:\n",
        "            recipe['ingredients'] = [clean_text(ingredient) for ingredient in recipe['ingredients'] if ingredient]\n",
        "        if 'instructions' in recipe and recipe['instructions']:\n",
        "            recipe['instructions'] = clean_text(recipe['instructions'])\n",
        "    return dataset\n",
        "\n",
        "def recipe_validate_required_fields(recipe):\n",
        "    required_keys = ['title', 'ingredients', 'instructions']\n",
        "    if not recipe:\n",
        "        return False\n",
        "    for required_key in required_keys:\n",
        "        if not recipe.get(required_key):\n",
        "            return False\n",
        "        if isinstance(recipe[required_key], list) and not recipe[required_key]:\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "3-zs_fJUhBWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "  dataset_file_names = [\n",
        "    'recipes_raw_nosource_ar.json',\n",
        "    # 'recipes_raw_nosource_epi.json',\n",
        "    # 'recipes_raw_nosource_fn.json',\n",
        "  ]\n",
        "\n",
        "  dataset = []\n",
        "  for dataset_file_name in dataset_file_names:\n",
        "    dataset_file_path = os.path.join('recipe_box', dataset_file_name)\n",
        "    with open(dataset_file_path, 'r') as dataset_file:\n",
        "      json_data_dict = json.load(dataset_file)\n",
        "      json_data_list = list(json_data_dict.values())\n",
        "      json_data_list = [recipe for recipe in json_data_list if recipe_validate_required_fields(recipe)]\n",
        "      cleaned_data = clean_dataset(json_data_list)\n",
        "      dataset.extend(cleaned_data)\n",
        "      print_dataset_info(dataset_file_path, cleaned_data)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def print_dataset_info(file_path, json_data):\n",
        "  print(file_path)\n",
        "  print('samples size:', len(json_data))\n",
        "  if json_data:\n",
        "    print('title:', json_data[0]['title'])\n",
        "    print('ingredients:', json_data[0]['ingredients'])\n",
        "    print('instructions:', json_data[0]['instructions'])\n",
        "    print()\n",
        "\n",
        "dataset_raw = load_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plKDXbUqDcA_",
        "outputId": "5c2bc44e-7ac7-44f4-fa10-f162a9d82b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recipe_box/recipes_raw_nosource_ar.json\n",
            "samples size: 39522\n",
            "title: Slow Cooker Chicken and Dumplings\n",
            "ingredients: ['4 skinless, boneless chicken breast halves', '2 tablespoons butter', '2 (10.75 ounce) cans condensed cream of chicken soup', '1 onion, finely diced', '2 (10 ounce) packages refrigerated biscuit dough, torn into pieces', '']\n",
            "instructions: Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover. Cover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 5000\n",
        "max_sequence_length = 500  # Maximum length of sequences\n",
        "\n",
        "# Tokenizer configuration\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "all_texts = [recipe['ingredients'] + [recipe['instructions']] for recipe in dataset_raw]\n",
        "all_texts = [' '.join(texts) for texts in all_texts]  # Combine ingredients and instructions for tokenization\n",
        "tokenizer.fit_on_texts(all_texts)\n",
        "\n",
        "# Function to prepare sequences\n",
        "def prepare_sequences(texts, tokenizer, max_sequence_length):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "    return padded_sequences\n",
        "\n",
        "# Preparing data\n",
        "ingredients_seqs = [' '.join(recipe['ingredients']) for recipe in dataset_raw]\n",
        "instructions_seqs = [recipe['instructions'] for recipe in dataset_raw]\n",
        "\n",
        "X = prepare_sequences(ingredients_seqs, tokenizer, max_sequence_length)\n",
        "y = prepare_sequences(instructions_seqs, tokenizer, max_sequence_length)\n",
        "\n",
        "# Splitting data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"X_train shape:\", X_train.shape)  # Should be (num_samples, sequence_length)\n",
        "print(\"y_train shape:\", y_train.shape)  # Should also be (num_samples, sequence_length)\n"
      ],
      "metadata": {
        "id": "S0I-rvytLHGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "950ab0fc-9bd4-4f09-e85d-713531462b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (31617, 500)\n",
            "y_train shape: (31617, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Model configuration\n",
        "\n",
        "embedding_dim = 256\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_sequence_length),\n",
        "    LSTM(256, return_sequences=True),\n",
        "    # LSTM(32, return_sequences=True),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gk3EMNJk3SO",
        "outputId": "f0c4f812-fcc9-4856-aaac-5713f1824605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, 500, 256)          1280000   \n",
            "                                                                 \n",
            " lstm_14 (LSTM)              (None, 500, 256)          525312    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 500, 5000)         1285000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3090312 (11.79 MB)\n",
            "Trainable params: 3090312 (11.79 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "EYmYM9jpk6KW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb45b37-2204-4678-ed9a-d60003c48475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "495/495 [==============================] - 115s 228ms/step - loss: 1.6713 - accuracy: 0.7953 - val_loss: 1.3232 - val_accuracy: 0.8010\n",
            "Epoch 2/10\n",
            "495/495 [==============================] - 119s 241ms/step - loss: 1.2973 - accuracy: 0.8016 - val_loss: 1.2684 - val_accuracy: 0.8037\n",
            "Epoch 3/10\n",
            "495/495 [==============================] - 119s 241ms/step - loss: 1.2668 - accuracy: 0.8027 - val_loss: 1.2514 - val_accuracy: 0.8041\n",
            "Epoch 4/10\n",
            "495/495 [==============================] - 120s 241ms/step - loss: 1.2499 - accuracy: 0.8034 - val_loss: 1.2427 - val_accuracy: 0.8050\n",
            "Epoch 5/10\n",
            "495/495 [==============================] - 119s 241ms/step - loss: 1.2371 - accuracy: 0.8040 - val_loss: 1.2304 - val_accuracy: 0.8051\n",
            "Epoch 6/10\n",
            "495/495 [==============================] - 119s 241ms/step - loss: 1.2239 - accuracy: 0.8044 - val_loss: 1.2257 - val_accuracy: 0.8054\n",
            "Epoch 7/10\n",
            "495/495 [==============================] - 113s 229ms/step - loss: 1.2121 - accuracy: 0.8047 - val_loss: 1.2172 - val_accuracy: 0.8057\n",
            "Epoch 8/10\n",
            "495/495 [==============================] - 119s 241ms/step - loss: 1.2013 - accuracy: 0.8050 - val_loss: 1.2086 - val_accuracy: 0.8057\n",
            "Epoch 9/10\n",
            "495/495 [==============================] - 113s 228ms/step - loss: 1.1926 - accuracy: 0.8052 - val_loss: 1.2003 - val_accuracy: 0.8059\n",
            "Epoch 10/10\n",
            "495/495 [==============================] - 120s 242ms/step - loss: 1.1838 - accuracy: 0.8054 - val_loss: 1.1962 - val_accuracy: 0.8058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_ingredients = \"chicken egg butter\"\n",
        "\n",
        "sample_seq = tokenizer.texts_to_sequences([sample_ingredients])\n",
        "sample_padded = pad_sequences(sample_seq, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "prediction = model.predict(sample_padded)\n",
        "\n",
        "predicted_instruction_idx = prediction.argmax(axis=-1)[0]\n",
        "predicted_instruction = tokenizer.sequences_to_texts([predicted_instruction_idx])\n",
        "\n",
        "print(\"Generated Cooking Instructions:\")\n",
        "print(predicted_instruction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZDnmB7LrZg8",
        "outputId": "18c9ba60-54ed-4aa9-9e45-be104b147013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 374ms/step\n",
            "Generated Cooking Instructions:\n",
            "['preheat oven to 350 degrees f degrees c a a a a a the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>']\n"
          ]
        }
      ]
    }
  ]
}